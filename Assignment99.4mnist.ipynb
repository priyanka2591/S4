{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment99.4mnist.ipynb","provenance":[{"file_id":"1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx","timestamp":1622235451522}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT"},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","\n","        # second convolution block\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","\n","        # third convolution block\n","        self.conv5 = nn.Conv2d(256, 512, 3)\n","        self.conv6 = nn.Conv2d(512, 1024, 3)\n","\n","        # fourth convolution block\n","        self.conv7 = nn.Conv2d(1024, 10, 3)\n","\n","    def forward(self, x):\n","        # first convolution block\n","        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n","        \n","        # second convoluton block\n","        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n","        \n","        # third convoluton block\n","        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n","        \n","        # fourth convoluton block\n","        x = F.relu(self.conv7(x))\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdydjYTZFyi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622232613569,"user_tz":-330,"elapsed":5107,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"1bb45334-4cf7-41d7-f016-73aad72ac308"},"source":["!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","            Conv2d-2           [-1, 64, 28, 28]          18,496\n","         MaxPool2d-3           [-1, 64, 14, 14]               0\n","            Conv2d-4          [-1, 128, 14, 14]          73,856\n","            Conv2d-5          [-1, 256, 14, 14]         295,168\n","         MaxPool2d-6            [-1, 256, 7, 7]               0\n","            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n","            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n","            Conv2d-9             [-1, 10, 1, 1]          92,170\n","================================================================\n","Total params: 6,379,786\n","Trainable params: 6,379,786\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.51\n","Params size (MB): 24.34\n","Estimated Total Size (MB): 25.85\n","----------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH"},"source":["\n","torch.manual_seed(1)\n","batch_size = 128\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0KTAoR_E4dL"},"source":["from tqdm import tqdm\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    # pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","    # pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1Yr5731SyJJ"},"source":["#### Customized model 1"]},{"cell_type":"markdown","metadata":{"id":"vnljr-AQf4XV"},"source":["1. The model consists of two convolutional layers 32 -> 64 with 3x3 kernel \n","2. GAP layer has been added by using the average pooling (28) over the size of the entire dimension of the input from the previous layer\n","3. This is then passed through a Dense layer which is the output layer in this case\n","4. batch normalization is done after every CNN layer \n","5. no dropouts have been implemented in this model"]},{"cell_type":"code","metadata":{"id":"7HQXxIuFOa4d"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  #Input -32x28x28 Output - 64x28x28 \n","        self.batch2 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.AvgPool2d(28) # global average pooling layer  input = 64x28x28 output  = 64x1x1 \n","        self.linear1 = nn.Linear(64,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10rjc3GjR37m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622213193730,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"a9dc9c2a-7605-48e1-f683-96dcc15190ce"},"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","       BatchNorm2d-2           [-1, 32, 28, 28]              64\n","            Conv2d-3           [-1, 64, 28, 28]          18,496\n","       BatchNorm2d-4           [-1, 64, 28, 28]             128\n","         AvgPool2d-5             [-1, 64, 1, 1]               0\n","            Linear-6                   [-1, 10]             650\n","================================================================\n","Total params: 19,658\n","Trainable params: 19,658\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.15\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 1.23\n","----------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622213396124,"user_tz":-330,"elapsed":196014,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"a1df5d60-6d83-43f7-f5c6-12ba10d8aa3e"},"source":["\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 1.2356, Accuracy: 6852/10000 (69%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.7770, Accuracy: 8250/10000 (82%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.6064, Accuracy: 8197/10000 (82%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.5183, Accuracy: 8533/10000 (85%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.2635, Accuracy: 9364/10000 (94%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.2477, Accuracy: 9456/10000 (95%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.2153, Accuracy: 9475/10000 (95%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.3899, Accuracy: 8859/10000 (89%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.1919, Accuracy: 9468/10000 (95%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.2169, Accuracy: 9395/10000 (94%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.1571, Accuracy: 9614/10000 (96%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.2268, Accuracy: 9329/10000 (93%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.1494, Accuracy: 9596/10000 (96%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.1305, Accuracy: 9654/10000 (97%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.1454, Accuracy: 9596/10000 (96%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.1144, Accuracy: 9683/10000 (97%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.1201, Accuracy: 9674/10000 (97%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.1574, Accuracy: 9554/10000 (96%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.1105, Accuracy: 9709/10000 (97%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GhXBPaoheKYN"},"source":["#### Customized model 2"]},{"cell_type":"markdown","metadata":{"id":"vrObPnhQgRuf"},"source":["1. The model consists of three convolutional layers 32 -> 64 with 3x3 kernel \n","2. GAP layer has been added by using the average pooling (28) over the size of the entire dimension of the input from the previous layer\n","3. This is then passed through a Dense layer which is the output layer in this case\n","4. batch normalization is done after every CNN layer \n","5. dropouts 0.20 have been implemented in this model after all cnn layers after batch normalization has been done"]},{"cell_type":"code","metadata":{"id":"So5uk4EkHW6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622213785268,"user_tz":-330,"elapsed":36709,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"e7a3ab9e-65ab-4f5a-fc11-aed8b9705e0a"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  #Input -32x28x28 Output - 64x28x28 \n","        self.batch2 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.AvgPool2d(28) # global average pooling layer  input = 64x28x28 output  = 64x1x1 \n","        self.linear1 = nn.Linear(64,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = F.dropout2d(x,0.2)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = F.dropout2d(x,0.2)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","  \n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","       BatchNorm2d-2           [-1, 32, 28, 28]              64\n","            Conv2d-3           [-1, 64, 28, 28]          18,496\n","       BatchNorm2d-4           [-1, 64, 28, 28]             128\n","         AvgPool2d-5             [-1, 64, 1, 1]               0\n","            Linear-6                   [-1, 10]             650\n","================================================================\n","Total params: 19,658\n","Trainable params: 19,658\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.15\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 1.23\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 1.4633, Accuracy: 5058/10000 (51%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 1.4338, Accuracy: 4596/10000 (46%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 1.0014, Accuracy: 6941/10000 (69%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.8203, Accuracy: 7596/10000 (76%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.5766, Accuracy: 8446/10000 (84%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.4935, Accuracy: 8780/10000 (88%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.4112, Accuracy: 8858/10000 (89%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.3805, Accuracy: 9012/10000 (90%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.3142, Accuracy: 9145/10000 (91%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.2782, Accuracy: 9234/10000 (92%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.2633, Accuracy: 9295/10000 (93%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.2599, Accuracy: 9277/10000 (93%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.2416, Accuracy: 9393/10000 (94%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.2214, Accuracy: 9379/10000 (94%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.2436, Accuracy: 9338/10000 (93%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.2094, Accuracy: 9410/10000 (94%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.2251, Accuracy: 9355/10000 (94%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.1981, Accuracy: 9456/10000 (95%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.1861, Accuracy: 9479/10000 (95%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqJbrnxYgxwr"},"source":["### Customized model 3\n","1. The model consists of three convolutional layers 32 -> 64 with 3x3 kernel \n","2. GAP layer has been added by using the average pooling (28) over the size of the entire dimension of the input from the previous layer\n","3. This is then passed through a Dense layer which is the output layer in this case\n","4. batch normalization is done after every CNN layer \n","5. dropouts 0.10 have been implemented in this model after the last convolutional layers after batch normalization has been done"]},{"cell_type":"code","metadata":{"id":"iX0mhsgtdvGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622215077449,"user_tz":-330,"elapsed":197729,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"1e58bf81-f4b5-484b-ff48-68c5b67de22f"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  #Input -32x28x28 Output - 64x28x28 \n","        self.batch2 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.AvgPool2d(28) # global average pooling layer  input = 64x28x28 output  = 64x1x1 \n","        self.linear1 = nn.Linear(64,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = F.dropout2d(x,0.1)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","  \n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","       BatchNorm2d-2           [-1, 32, 28, 28]              64\n","            Conv2d-3           [-1, 64, 28, 28]          18,496\n","       BatchNorm2d-4           [-1, 64, 28, 28]             128\n","         AvgPool2d-5             [-1, 64, 1, 1]               0\n","            Linear-6                   [-1, 10]             650\n","================================================================\n","Total params: 19,658\n","Trainable params: 19,658\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.15\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 1.23\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 1.2385, Accuracy: 6510/10000 (65%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.7689, Accuracy: 8340/10000 (83%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.5166, Accuracy: 8757/10000 (88%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.3756, Accuracy: 9181/10000 (92%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.6206, Accuracy: 7800/10000 (78%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.2189, Accuracy: 9480/10000 (95%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.2059, Accuracy: 9482/10000 (95%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.1769, Accuracy: 9566/10000 (96%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.2201, Accuracy: 9432/10000 (94%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.1672, Accuracy: 9568/10000 (96%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.1562, Accuracy: 9595/10000 (96%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.1523, Accuracy: 9582/10000 (96%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.1465, Accuracy: 9614/10000 (96%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.1624, Accuracy: 9549/10000 (95%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.1338, Accuracy: 9630/10000 (96%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.1425, Accuracy: 9596/10000 (96%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.1337, Accuracy: 9632/10000 (96%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.1296, Accuracy: 9659/10000 (97%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.1191, Accuracy: 9679/10000 (97%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e-ctgpIrhwc4"},"source":["### Customized model 4"]},{"cell_type":"code","metadata":{"id":"jHVQU476fEVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622215670810,"user_tz":-330,"elapsed":202964,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"dd2825c5-e54e-4f5d-e04e-f875b5c1df24"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch2 = nn.BatchNorm2d(16)\n","        self.conv3= nn.Conv2d(16, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch3 = nn.BatchNorm2d(32)\n","        self.conv4= nn.Conv2d(32, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch4 = nn.BatchNorm2d(32)\n","\n","        # self.conv4 = nn.Conv2d(32, 64, 3, padding=1)  #Input -32x28x28 Output - 32x28x28 \n","        # self.batch4 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.AvgPool2d(28) # global average pooling layer  input = 64x28x28 output  = 64x1x1 \n","        self.linear1 = nn.Linear(32,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = F.dropout2d(x,0.05)\n","\n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","            Conv2d-3           [-1, 16, 28, 28]           1,168\n","       BatchNorm2d-4           [-1, 16, 28, 28]              32\n","            Conv2d-5           [-1, 32, 28, 28]           4,640\n","       BatchNorm2d-6           [-1, 32, 28, 28]              64\n","            Conv2d-7           [-1, 32, 28, 28]           9,248\n","       BatchNorm2d-8           [-1, 32, 28, 28]              64\n","         AvgPool2d-9             [-1, 32, 1, 1]               0\n","           Linear-10                   [-1, 10]             330\n","================================================================\n","Total params: 15,642\n","Trainable params: 15,642\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.05\n","Params size (MB): 0.06\n","Estimated Total Size (MB): 1.12\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.6199, Accuracy: 8373/10000 (84%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.2843, Accuracy: 9335/10000 (93%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.1709, Accuracy: 9582/10000 (96%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.1251, Accuracy: 9665/10000 (97%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.1716, Accuracy: 9525/10000 (95%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.1043, Accuracy: 9713/10000 (97%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.1034, Accuracy: 9700/10000 (97%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0779, Accuracy: 9798/10000 (98%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0747, Accuracy: 9785/10000 (98%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.1030, Accuracy: 9686/10000 (97%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.1008, Accuracy: 9709/10000 (97%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0805, Accuracy: 9780/10000 (98%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0665, Accuracy: 9812/10000 (98%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0812, Accuracy: 9761/10000 (98%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0622, Accuracy: 9818/10000 (98%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0688, Accuracy: 9784/10000 (98%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0641, Accuracy: 9819/10000 (98%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0585, Accuracy: 9826/10000 (98%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0547, Accuracy: 9840/10000 (98%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PutY9JUEmRql"},"source":["### Customized model 5"]},{"cell_type":"code","metadata":{"id":"1-MLNAXdj4lj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622217312853,"user_tz":-330,"elapsed":195678,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"d473a713-911c-4d1a-f34b-dabdc9990f1e"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch2 = nn.BatchNorm2d(16)\n","        self.conv3= nn.Conv2d(16, 32, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch3 = nn.BatchNorm2d(32)\n","        self.conv4= nn.Conv2d(32, 46, 3, padding=1) #Input -1x28x28 Output - 32x28x28 \n","        self.batch4 = nn.BatchNorm2d(46)\n","\n","        # self.conv4 = nn.Conv2d(32, 64, 3, padding=1)  #Input -32x28x28 Output - 32x28x28 \n","        # self.batch4 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.AvgPool2d(28) # global average pooling layer  input = 64x28x28 output  = 64x1x1 \n","        self.linear1 = nn.Linear(46,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        # x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        # x = self.batch3(x)\n","        x = F.dropout2d(x,0.005)\n","\n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","            Conv2d-2           [-1, 16, 28, 28]           1,168\n","       BatchNorm2d-3           [-1, 16, 28, 28]              32\n","            Conv2d-4           [-1, 32, 28, 28]           4,640\n","            Conv2d-5           [-1, 46, 28, 28]          13,294\n","       BatchNorm2d-6           [-1, 46, 28, 28]              92\n","         AvgPool2d-7             [-1, 46, 1, 1]               0\n","            Linear-8                   [-1, 10]             470\n","================================================================\n","Total params: 19,776\n","Trainable params: 19,776\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.98\n","Params size (MB): 0.08\n","Estimated Total Size (MB): 1.06\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.4681, Accuracy: 9045/10000 (90.45%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.2600, Accuracy: 9318/10000 (93.18%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.1490, Accuracy: 9596/10000 (95.96%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.1167, Accuracy: 9698/10000 (96.98%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.1435, Accuracy: 9592/10000 (95.92%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.1708, Accuracy: 9466/10000 (94.66%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0931, Accuracy: 9739/10000 (97.39%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0983, Accuracy: 9723/10000 (97.23%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0773, Accuracy: 9773/10000 (97.73%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0662, Accuracy: 9801/10000 (98.01%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0787, Accuracy: 9756/10000 (97.56%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0579, Accuracy: 9831/10000 (98.31%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0648, Accuracy: 9809/10000 (98.09%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0663, Accuracy: 9809/10000 (98.09%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0718, Accuracy: 9793/10000 (97.93%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0602, Accuracy: 9827/10000 (98.27%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0590, Accuracy: 9819/10000 (98.19%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0677, Accuracy: 9785/10000 (97.85%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0579, Accuracy: 9819/10000 (98.19%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j5xtjVqDsuEi"},"source":["### Customized model 6"]},{"cell_type":"code","metadata":{"id":"sGJ2MuLmmFeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622218494091,"user_tz":-330,"elapsed":197488,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"4ab2e85f-44ca-4969-fbdb-a23003235e07"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -8x28x28 Output - 16x28x28 \n","        self.batch2 = nn.BatchNorm2d(16)\n","        \n","        self.poola = nn.MaxPool2d(2, 2) # input 16x28x28 output 16x14x14\n","\n","        self.conv3= nn.Conv2d(16, 32, 3, padding=1) #Input -16x14x14 Output - 32x14x14 \n","        self.batch3 = nn.BatchNorm2d(32)\n","        self.conv4= nn.Conv2d(32, 46, 3, padding=1) #Input -32x14x14 Output - 46x14x14 \n","        self.batch4 = nn.BatchNorm2d(46)\n","\n","        self.pool1 = nn.AvgPool2d(14) # global average pooling layer  input = 46x14x14 output  = 46x1x1 \n","        self.linear1 = nn.Linear(46,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        # x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = self.poola(x)\n","        \n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        # x = self.batch3(x)\n","        x = F.dropout2d(x,0.005)\n","\n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","            Conv2d-2           [-1, 16, 28, 28]           1,168\n","       BatchNorm2d-3           [-1, 16, 28, 28]              32\n","         MaxPool2d-4           [-1, 16, 14, 14]               0\n","            Conv2d-5           [-1, 32, 14, 14]           4,640\n","            Conv2d-6           [-1, 46, 14, 14]          13,294\n","       BatchNorm2d-7           [-1, 46, 14, 14]              92\n","         AvgPool2d-8             [-1, 46, 1, 1]               0\n","            Linear-9                   [-1, 10]             470\n","================================================================\n","Total params: 19,776\n","Trainable params: 19,776\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.45\n","Params size (MB): 0.08\n","Estimated Total Size (MB): 0.53\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.2472, Accuracy: 9422/10000 (94.22%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.2104, Accuracy: 9416/10000 (94.16%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.1239, Accuracy: 9668/10000 (96.68%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0784, Accuracy: 9793/10000 (97.93%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0850, Accuracy: 9766/10000 (97.66%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0472, Accuracy: 9866/10000 (98.66%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0489, Accuracy: 9864/10000 (98.64%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0465, Accuracy: 9865/10000 (98.65%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0431, Accuracy: 9873/10000 (98.73%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0410, Accuracy: 9878/10000 (98.78%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0378, Accuracy: 9892/10000 (98.92%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0340, Accuracy: 9896/10000 (98.96%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0413, Accuracy: 9887/10000 (98.87%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0362, Accuracy: 9880/10000 (98.80%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0330, Accuracy: 9903/10000 (99.03%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0331, Accuracy: 9899/10000 (98.99%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0328, Accuracy: 9885/10000 (98.85%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0290, Accuracy: 9914/10000 (99.14%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0309, Accuracy: 9895/10000 (98.95%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yYwmAGs42xhe"},"source":["### Customized model 7"]},{"cell_type":"code","metadata":{"id":"NOqsADXssI-Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622219461643,"user_tz":-330,"elapsed":201855,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"4a3ed932-a66c-43a2-c2cb-6e466f7c1786"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","        self.poola = nn.MaxPool2d(2, 2) # input  - 8x28x28 output - 8x14x14\n","\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -8x14x14 Output - 16x14x14 \n","        self.batch2 = nn.BatchNorm2d(16)        \n","\n","        self.conv3= nn.Conv2d(16, 32, 3, padding=1) #Input -16x14x14 Output - 32x14x14 \n","        self.batch3 = nn.BatchNorm2d(32)\n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        self.conv4= nn.Conv2d(32, 46, 3, padding=1) #Input -32x7x7 Output - 46x7x7 \n","        self.batch4 = nn.BatchNorm2d(46)\n","\n","        self.pool1 = nn.AvgPool2d(7) # global average pooling layer  input = 46x14x14 output  = 46x1x1 \n","        self.linear1 = nn.Linear(46,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = self.poola(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        \n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = self.poolb(x)\n","        x = F.dropout2d(x,0.005)\n","\n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","         MaxPool2d-3            [-1, 8, 14, 14]               0\n","            Conv2d-4           [-1, 16, 14, 14]           1,168\n","       BatchNorm2d-5           [-1, 16, 14, 14]              32\n","            Conv2d-6           [-1, 32, 14, 14]           4,640\n","       BatchNorm2d-7           [-1, 32, 14, 14]              64\n","         MaxPool2d-8             [-1, 32, 7, 7]               0\n","            Conv2d-9             [-1, 46, 7, 7]          13,294\n","      BatchNorm2d-10             [-1, 46, 7, 7]              92\n","        AvgPool2d-11             [-1, 46, 1, 1]               0\n","           Linear-12                   [-1, 10]             470\n","================================================================\n","Total params: 19,856\n","Trainable params: 19,856\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.30\n","Params size (MB): 0.08\n","Estimated Total Size (MB): 0.38\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.1262, Accuracy: 9676/10000 (96.76%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0961, Accuracy: 9726/10000 (97.26%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0571, Accuracy: 9836/10000 (98.36%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0555, Accuracy: 9839/10000 (98.39%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0459, Accuracy: 9870/10000 (98.70%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0732, Accuracy: 9774/10000 (97.74%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0425, Accuracy: 9869/10000 (98.69%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0374, Accuracy: 9885/10000 (98.85%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0388, Accuracy: 9881/10000 (98.81%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0366, Accuracy: 9882/10000 (98.82%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0326, Accuracy: 9896/10000 (98.96%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0333, Accuracy: 9894/10000 (98.94%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0317, Accuracy: 9896/10000 (98.96%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0341, Accuracy: 9898/10000 (98.98%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0343, Accuracy: 9894/10000 (98.94%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0295, Accuracy: 9904/10000 (99.04%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0317, Accuracy: 9904/10000 (99.04%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0309, Accuracy: 9900/10000 (99.00%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0293, Accuracy: 9904/10000 (99.04%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-3e2ZBY23db"},"source":["### Customized model 8"]},{"cell_type":"code","metadata":{"id":"Tp0fImMpwqgW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622219897119,"user_tz":-330,"elapsed":202186,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"7a0c107e-1726-4d9d-c578-d9781a8e18d3"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","        self.poola = nn.MaxPool2d(2, 2)\n","\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -8x14x14 Output - 16x14x14 \n","        self.batch2 = nn.BatchNorm2d(16)        \n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        self.conv3= nn.Conv2d(16, 32, 3, padding=1) #Input -16x7x7 Output - 32x7x7 \n","        self.batch3 = nn.BatchNorm2d(32)\n","        self.poolc = nn.MaxPool2d(2, 2)\n","\n","        self.conv4= nn.Conv2d(32, 46, 3, padding=1) #Input -32x3x3 Output - 46x3x3 \n","        self.batch4 = nn.BatchNorm2d(46)\n","        self.pool1 = nn.AvgPool2d(3) # global average pooling layer  input = 46x14x14 output  = 46x1x1 \n","        \n","        self.linear1 = nn.Linear(46,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = self.poola(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = self.poolb(x)\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = self.poolc(x)\n","        x = F.dropout2d(x,0.005)\n","\n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","         MaxPool2d-3            [-1, 8, 14, 14]               0\n","            Conv2d-4           [-1, 16, 14, 14]           1,168\n","       BatchNorm2d-5           [-1, 16, 14, 14]              32\n","         MaxPool2d-6             [-1, 16, 7, 7]               0\n","            Conv2d-7             [-1, 32, 7, 7]           4,640\n","       BatchNorm2d-8             [-1, 32, 7, 7]              64\n","         MaxPool2d-9             [-1, 32, 3, 3]               0\n","           Conv2d-10             [-1, 46, 3, 3]          13,294\n","      BatchNorm2d-11             [-1, 46, 3, 3]              92\n","        AvgPool2d-12             [-1, 46, 1, 1]               0\n","           Linear-13                   [-1, 10]             470\n","================================================================\n","Total params: 19,856\n","Trainable params: 19,856\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.19\n","Params size (MB): 0.08\n","Estimated Total Size (MB): 0.27\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0705, Accuracy: 9795/10000 (97.95%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0486, Accuracy: 9871/10000 (98.71%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0414, Accuracy: 9875/10000 (98.75%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0526, Accuracy: 9829/10000 (98.29%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0380, Accuracy: 9875/10000 (98.75%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0337, Accuracy: 9889/10000 (98.89%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0347, Accuracy: 9895/10000 (98.95%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0351, Accuracy: 9889/10000 (98.89%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0291, Accuracy: 9899/10000 (98.99%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0307, Accuracy: 9899/10000 (98.99%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0325, Accuracy: 9891/10000 (98.91%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0336, Accuracy: 9886/10000 (98.86%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0320, Accuracy: 9893/10000 (98.93%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0314, Accuracy: 9907/10000 (99.07%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0313, Accuracy: 9905/10000 (99.05%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0342, Accuracy: 9887/10000 (98.87%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0296, Accuracy: 9905/10000 (99.05%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0305, Accuracy: 9905/10000 (99.05%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0306, Accuracy: 9904/10000 (99.04%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UXUzclho26s5"},"source":["### Customized model 9"]},{"cell_type":"code","metadata":{"id":"PaaaRUlJ0Wi9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622230984018,"user_tz":-330,"elapsed":206759,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"bc71c9bd-7d50-4cf1-a2b4-ed26f5c7e903"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(8)\n","\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1) #Input -8x28x28 Output - 16x28x28 \n","        self.batch2 = nn.BatchNorm2d(16)        \n","        self.poola = nn.MaxPool2d(2, 2)\n","\n","        self.conv3= nn.Conv2d(16, 24, 3, padding=1) #Input -16x14x14 Output - 24x14x14 \n","        self.batch3 = nn.BatchNorm2d(24)\n","\n","        self.conv4= nn.Conv2d(24, 36, 3, padding=1) #Input -24x14x14 Output - 36x14x14 \n","        self.batch4 = nn.BatchNorm2d(36)\n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        self.pool1 = nn.AvgPool2d(7) # global average pooling layer  input = 38x7x7 output  = 38x1x1         \n","        self.linear1 = nn.Linear(36,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = self.poola(x)\n","        x = F.dropout2d(x,0.005)\n","\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        \n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","        x = self.poolb(x)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","            Conv2d-3           [-1, 16, 28, 28]           1,168\n","       BatchNorm2d-4           [-1, 16, 28, 28]              32\n","         MaxPool2d-5           [-1, 16, 14, 14]               0\n","            Conv2d-6           [-1, 24, 14, 14]           3,480\n","       BatchNorm2d-7           [-1, 24, 14, 14]              48\n","            Conv2d-8           [-1, 36, 14, 14]           7,812\n","       BatchNorm2d-9           [-1, 36, 14, 14]              72\n","        MaxPool2d-10             [-1, 36, 7, 7]               0\n","        AvgPool2d-11             [-1, 36, 1, 1]               0\n","           Linear-12                   [-1, 10]             370\n","================================================================\n","Total params: 13,078\n","Trainable params: 13,078\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.50\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 0.56\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.1147, Accuracy: 9712/10000 (97.12%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0717, Accuracy: 9798/10000 (97.98%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0578, Accuracy: 9841/10000 (98.41%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0527, Accuracy: 9861/10000 (98.61%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0504, Accuracy: 9853/10000 (98.53%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0400, Accuracy: 9872/10000 (98.72%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0388, Accuracy: 9880/10000 (98.80%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0438, Accuracy: 9867/10000 (98.67%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0374, Accuracy: 9892/10000 (98.92%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0403, Accuracy: 9880/10000 (98.80%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0387, Accuracy: 9877/10000 (98.77%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0399, Accuracy: 9868/10000 (98.68%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0313, Accuracy: 9898/10000 (98.98%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0359, Accuracy: 9889/10000 (98.89%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0349, Accuracy: 9892/10000 (98.92%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0362, Accuracy: 9885/10000 (98.85%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0338, Accuracy: 9881/10000 (98.81%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0309, Accuracy: 9892/10000 (98.92%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0334, Accuracy: 9892/10000 (98.92%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4jIjeRXUze7"},"source":["### Customized 10"]},{"cell_type":"code","metadata":{"id":"UJFkRvUV2ALH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622233338218,"user_tz":-330,"elapsed":201845,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"f15c0e50-2a68-457a-ca17-8ae47f37ad81"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 10, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(10)\n","\n","        self.conv2 = nn.Conv2d(10, 20, 3, padding=1) #Input -8x28x28 Output - 16x28x28 \n","        self.batch2 = nn.BatchNorm2d(20)        \n","        self.poola = nn.MaxPool2d(2, 2)\n","\n","        self.conv3= nn.Conv2d(20, 30, 3, padding=1) #Input -16x14x14 Output - 24x14x14 \n","        self.batch3 = nn.BatchNorm2d(30)\n","        self.poolb0 = nn.MaxPool2d(2, 2)\n","\n","        self.conv4= nn.Conv2d(30, 40, 3, padding=1) #Input -24x14x14 Output - 36x14x14 \n","        self.batch4 = nn.BatchNorm2d(40)\n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        self.pool1 = nn.AvgPool2d(3) # global average pooling layer  input = 38x7x7 output  = 38x1x1         \n","        self.linear1 = nn.Linear(40,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = self.poola(x)\n","        x = F.dropout2d(x,0.01)\n","\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = self.poolb0(x)\n","        \n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","        x = self.poolb(x)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 10, 28, 28]             100\n","       BatchNorm2d-2           [-1, 10, 28, 28]              20\n","            Conv2d-3           [-1, 20, 28, 28]           1,820\n","       BatchNorm2d-4           [-1, 20, 28, 28]              40\n","         MaxPool2d-5           [-1, 20, 14, 14]               0\n","            Conv2d-6           [-1, 30, 14, 14]           5,430\n","       BatchNorm2d-7           [-1, 30, 14, 14]              60\n","         MaxPool2d-8             [-1, 30, 7, 7]               0\n","            Conv2d-9             [-1, 40, 7, 7]          10,840\n","      BatchNorm2d-10             [-1, 40, 7, 7]              80\n","        MaxPool2d-11             [-1, 40, 3, 3]               0\n","        AvgPool2d-12             [-1, 40, 1, 1]               0\n","           Linear-13                   [-1, 10]             410\n","================================================================\n","Total params: 18,800\n","Trainable params: 18,800\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.52\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 0.60\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0694, Accuracy: 9825/10000 (98.25%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0497, Accuracy: 9857/10000 (98.57%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0392, Accuracy: 9886/10000 (98.86%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0331, Accuracy: 9895/10000 (98.95%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0307, Accuracy: 9903/10000 (99.03%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0301, Accuracy: 9901/10000 (99.01%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0257, Accuracy: 9912/10000 (99.12%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0264, Accuracy: 9909/10000 (99.09%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0315, Accuracy: 9898/10000 (98.98%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0269, Accuracy: 9902/10000 (99.02%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0262, Accuracy: 9909/10000 (99.09%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0269, Accuracy: 9920/10000 (99.20%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0249, Accuracy: 9916/10000 (99.16%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0230, Accuracy: 9920/10000 (99.20%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0260, Accuracy: 9911/10000 (99.11%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0260, Accuracy: 9920/10000 (99.20%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0257, Accuracy: 9917/10000 (99.17%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0268, Accuracy: 9910/10000 (99.10%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0243, Accuracy: 9914/10000 (99.14%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fQtpRNu3nWAH"},"source":["### Customized model 11\n"]},{"cell_type":"code","metadata":{"id":"dCuZzfTPYia-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622233539281,"user_tz":-330,"elapsed":201081,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"4030deac-58a4-4e4f-9c39-8c523e281b52"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 10, 3, padding=1) #Input -1x28x28 Output - 10x28x28 \n","        self.batch1 = nn.BatchNorm2d(10)\n","        \n","        # second convolution block\n","        self.conv2 = nn.Conv2d(10, 20, 3, padding=1) #Input -10x28x28 Output - 20x28x28 \n","        self.batch2 = nn.BatchNorm2d(20)        \n","        self.poola = nn.MaxPool2d(2, 2)\n","\n","        # third convolution block\n","        self.conv3= nn.Conv2d(20, 30, 3, padding=1) #Input -20x14x14 Output -30x14x14 \n","        self.batch3 = nn.BatchNorm2d(30)\n","        self.poolb0 = nn.MaxPool2d(2, 2)\n","\n","        # fourth convolution block\n","        self.conv4= nn.Conv2d(30, 40, 3, padding=1) #Input -30x14x14 Output - 40x7x7 \n","        self.batch4 = nn.BatchNorm2d(40)\n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        # GAP layer\n","        self.pool1 = nn.AvgPool2d(3) # global average pooling layer  input = 40x3x3 output  = 40x1x1     ? 3 because this is the input dimension of the iamge    \n","        # FC layer - > output layer  \n","        self.linear1 = nn.Linear(40,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = F.dropout2d(x,0.01) # first drop out ;a\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = F.dropout2d(x,0.01)\n","        x = self.poola(x)\n","        \n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = F.dropout2d(x,0.01)\n","        x = self.poolb0(x)\n","        \n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","        x = F.dropout2d(x,0.01)\n","        x = self.poolb(x)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 10, 28, 28]             100\n","       BatchNorm2d-2           [-1, 10, 28, 28]              20\n","            Conv2d-3           [-1, 20, 28, 28]           1,820\n","       BatchNorm2d-4           [-1, 20, 28, 28]              40\n","         MaxPool2d-5           [-1, 20, 14, 14]               0\n","            Conv2d-6           [-1, 30, 14, 14]           5,430\n","       BatchNorm2d-7           [-1, 30, 14, 14]              60\n","         MaxPool2d-8             [-1, 30, 7, 7]               0\n","            Conv2d-9             [-1, 40, 7, 7]          10,840\n","      BatchNorm2d-10             [-1, 40, 7, 7]              80\n","        MaxPool2d-11             [-1, 40, 3, 3]               0\n","        AvgPool2d-12             [-1, 40, 1, 1]               0\n","           Linear-13                   [-1, 10]             410\n","================================================================\n","Total params: 18,800\n","Trainable params: 18,800\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.52\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 0.60\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0771, Accuracy: 9795/10000 (97.95%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0507, Accuracy: 9862/10000 (98.62%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0403, Accuracy: 9873/10000 (98.73%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0330, Accuracy: 9898/10000 (98.98%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0343, Accuracy: 9893/10000 (98.93%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0270, Accuracy: 9921/10000 (99.21%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0284, Accuracy: 9911/10000 (99.11%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0288, Accuracy: 9909/10000 (99.09%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0287, Accuracy: 9901/10000 (99.01%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0258, Accuracy: 9908/10000 (99.08%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0260, Accuracy: 9917/10000 (99.17%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0244, Accuracy: 9928/10000 (99.28%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0276, Accuracy: 9916/10000 (99.16%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0246, Accuracy: 9926/10000 (99.26%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0242, Accuracy: 9921/10000 (99.21%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0244, Accuracy: 9922/10000 (99.22%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0248, Accuracy: 9926/10000 (99.26%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0244, Accuracy: 9923/10000 (99.23%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0233, Accuracy: 9927/10000 (99.27%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U6jBZdYFqBFo"},"source":["### Customized model 12"]},{"cell_type":"code","metadata":{"id":"aRN45Thub1rZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622233907874,"user_tz":-330,"elapsed":202374,"user":{"displayName":"Sabeesh E","photoUrl":"","userId":"15433255022771842152"}},"outputId":"e9601e45-8a2f-429c-f0a0-78037e6f62fc"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # first convolution block\n","        self.conv1 = nn.Conv2d(1, 10, 3, padding=1) #Input -1x28x28 Output - 8x28x28 \n","        self.batch1 = nn.BatchNorm2d(10)\n","\n","        self.conv2 = nn.Conv2d(10, 20, 3, padding=1) #Input -8x28x28 Output - 16x28x28 \n","        self.batch2 = nn.BatchNorm2d(20)        \n","        self.poola = nn.MaxPool2d(2, 2)\n","\n","        self.conv3= nn.Conv2d(20, 30, 3, padding=1) #Input -16x14x14 Output - 24x14x14 \n","        self.batch3 = nn.BatchNorm2d(30)\n","        self.poolb0 = nn.MaxPool2d(2, 2)\n","\n","        self.conv4= nn.Conv2d(30, 40, 3, padding=1) #Input -24x14x14 Output - 36x14x14 \n","        self.batch4 = nn.BatchNorm2d(40)\n","        self.poolb = nn.MaxPool2d(2, 2)\n","\n","        self.pool1 = nn.AvgPool2d(3) # global average pooling layer  input = 38x7x7 output  = 38x1x1         \n","        self.linear1 = nn.Linear(40,10)\n","        \n","    def forward(self, x):\n","        # first convolution block\n","       \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.batch1(x)\n","        x = F.dropout2d(x,0.05)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.batch2(x)\n","        x = F.dropout2d(x,0.05)\n","        x = self.poola(x)\n","        \n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.batch3(x)\n","        x = F.dropout2d(x,0.01)\n","        x = self.poolb0(x)\n","        \n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.batch4(x)\n","        x = F.dropout2d(x,0.01)\n","        x = self.poolb(x)\n","\n","        x = self.pool1(x)\n","        x  = x.squeeze()\n","        x = self.linear1(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))\n","\n","\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    print(f'Epoch number {epoch}')\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 10, 28, 28]             100\n","       BatchNorm2d-2           [-1, 10, 28, 28]              20\n","            Conv2d-3           [-1, 20, 28, 28]           1,820\n","       BatchNorm2d-4           [-1, 20, 28, 28]              40\n","         MaxPool2d-5           [-1, 20, 14, 14]               0\n","            Conv2d-6           [-1, 30, 14, 14]           5,430\n","       BatchNorm2d-7           [-1, 30, 14, 14]              60\n","         MaxPool2d-8             [-1, 30, 7, 7]               0\n","            Conv2d-9             [-1, 40, 7, 7]          10,840\n","      BatchNorm2d-10             [-1, 40, 7, 7]              80\n","        MaxPool2d-11             [-1, 40, 3, 3]               0\n","        AvgPool2d-12             [-1, 40, 1, 1]               0\n","           Linear-13                   [-1, 10]             410\n","================================================================\n","Total params: 18,800\n","Trainable params: 18,800\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.52\n","Params size (MB): 0.07\n","Estimated Total Size (MB): 0.60\n","----------------------------------------------------------------\n","Epoch number 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0814, Accuracy: 9772/10000 (97.72%)\n","\n","Epoch number 2\n","\n","Test set: Average loss: 0.0482, Accuracy: 9866/10000 (98.66%)\n","\n","Epoch number 3\n","\n","Test set: Average loss: 0.0453, Accuracy: 9862/10000 (98.62%)\n","\n","Epoch number 4\n","\n","Test set: Average loss: 0.0401, Accuracy: 9871/10000 (98.71%)\n","\n","Epoch number 5\n","\n","Test set: Average loss: 0.0339, Accuracy: 9903/10000 (99.03%)\n","\n","Epoch number 6\n","\n","Test set: Average loss: 0.0344, Accuracy: 9886/10000 (98.86%)\n","\n","Epoch number 7\n","\n","Test set: Average loss: 0.0364, Accuracy: 9884/10000 (98.84%)\n","\n","Epoch number 8\n","\n","Test set: Average loss: 0.0301, Accuracy: 9902/10000 (99.02%)\n","\n","Epoch number 9\n","\n","Test set: Average loss: 0.0309, Accuracy: 9898/10000 (98.98%)\n","\n","Epoch number 10\n","\n","Test set: Average loss: 0.0342, Accuracy: 9894/10000 (98.94%)\n","\n","Epoch number 11\n","\n","Test set: Average loss: 0.0280, Accuracy: 9910/10000 (99.10%)\n","\n","Epoch number 12\n","\n","Test set: Average loss: 0.0274, Accuracy: 9909/10000 (99.09%)\n","\n","Epoch number 13\n","\n","Test set: Average loss: 0.0298, Accuracy: 9895/10000 (98.95%)\n","\n","Epoch number 14\n","\n","Test set: Average loss: 0.0262, Accuracy: 9914/10000 (99.14%)\n","\n","Epoch number 15\n","\n","Test set: Average loss: 0.0267, Accuracy: 9915/10000 (99.15%)\n","\n","Epoch number 16\n","\n","Test set: Average loss: 0.0291, Accuracy: 9907/10000 (99.07%)\n","\n","Epoch number 17\n","\n","Test set: Average loss: 0.0287, Accuracy: 9907/10000 (99.07%)\n","\n","Epoch number 18\n","\n","Test set: Average loss: 0.0307, Accuracy: 9899/10000 (98.99%)\n","\n","Epoch number 19\n","\n","Test set: Average loss: 0.0284, Accuracy: 9910/10000 (99.10%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gOu2VYvjp94W"},"source":[""],"execution_count":null,"outputs":[]}]}